\chapter{Deployment}


\section{User feedback}
Once the application had been through several development iterations and was nearing completion, it was presented to the researchers for signing off, discussions about deployment and maintenance provisions. Only one major change was requested, to have the site password protected, as they wanted to restrict access to only trusted researchers working on the project, until the research was published.

Ideally a password system would allow for multiple users with different passwords, with some levels of account controls by an administrator. This would be quite an extensive bit of work to undergo as a new users model would have to be created and the user interface would have to be updated to reflect all of the these new controls. Due to the time restrictions on this project, this was not a feasible request to complete to a satisfactory level.

An alternative, but far inferior solution was instead implemented, simply adding a basic HTTP auth header to the application. This involved only adding a module (basic-auth-connect) to the application, and one line of code to enable it in the middleware of the app. This does provide password protection to the site to keep curious eyes out of the project, however it is very insecure as the password is stored on the server in plain text, as well as transmitted unencrypted over HTTP, since HTTPS hasn't been implemented for this service. 

\section{Deployment}
To deploy this project, an Ubuntu 16.04 Virtual Machine (VM) with three cores, 10GB of storage and 4GB of RAM was provisioned by the university. A user `candida' was then made for the project to be ran under and the release branch of the project cloned down to the machine from the Github repository. Node.js was then installed from the Ubuntu repositories, so that the system had access to the Node Package Manager (npm). This was then used to install nave\cite{nave}, m\cite{m}, and yarn\cite{yarn}. Three tools that help with the management of packages and versioning for Node.js, MongoDB and npm packages.

Nave was then used to install Node.js version 6.9.0, which is what this application was developed for, and m was used to install MongoDB 3.4.4. The dependencies listed in the package.json file were then installed to the node\_modules folder with yarn, the project could then be built with the build script defined in the package.json. This process is documented in the projects README.md file.

To generate the data for the database and import it to MongoDB, the seed script defined in the package.json was run, this runs the importation script using the data that is included in the repository to populate the database.

To ensure the stability of the service, it was put behind a Varnish\cite{varnish} cache. Doing this enabled caching on all of the responses from the web server to client requests, meaning that if a request has already had a response processed for it, then a cached version will be served from Varnish rather than adding load to the Node.js application. This speeds up the sites response time as data can be immediately returned, once it has been processed already.

Unfortunately a side effect of the last minute need to include basic HTTP auth, is that the benefits of putting the service behind Varnish are nullified, as Varnish won't cache content behind a basic HTTP auth. This another good reason to find a better solution to the password protection in the future.

A systemd\cite{systemd} service file was then written and installed on the server for the Node.js process, this enables the application to be controlled from systemd. This is done so that the application can be started at boot time, as well as monitored, restarted and stopped from the systemd interface; which is how all the other services on the VM are controlled.

One difficulty faced during this deployment was the versioning on MongoDB, provided by the OS on the VM. The application had been developed on Arch Linux which provides a recent version (3.4) of MongoDB in its package manager, however on the VM which was running Ubuntu, the package was for version 2.6, that doesn't support the new wiredTiger\cite{tiger} storage engine. 

The difference in versions was noted initially, and corrected by installing version 3.4. However when data was entered into the database, it was taking up around three times as much room, filling up the VM's disk space. 

To correct this a manual process of updating the storage engine and database directory in the MongoDB configuration was necessary. This enabled the data to be imported correctly, taking up the expected amount of storage. 

With the deployment complete, the README.md file in the project was updated to reflect the steps necessary to replicate a full deployment, so that other developers could replicate the production environment. 
